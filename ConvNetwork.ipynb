{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import sandbox, Out\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdir='/users/ngupta/PythonTools/DeepLearning'\n",
    "sys.path.append(cdir+\"/../utilities\")\n",
    "sys.path.append(cdir+\"/../Eval\")\n",
    "sys.path.append(cdir+\"/lib\")\n",
    "from SaveLoadModel import SaveLoadModel\n",
    "from HiddenLayer import HiddenLayer\n",
    "from LogReg import LogisticRegression\n",
    "import PrecisionRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.floatX = 'float32'\n",
    "d='/users/ngupta/PythonTools/DeepLearning/Classification/TextClassification/workDir/' \n",
    "hiddenLayerSizes=\"50\"\n",
    "wordVecLen=100\n",
    "inputSize=7\n",
    "globalfeCount=8\n",
    "CVectorSize=10\n",
    "CCLayerSize=0\n",
    "CCContext=2 \n",
    "n_out=3\n",
    "L2_reg = 0.001\n",
    "L1_reg = 0\n",
    "vecfile=d+'train.vec.pcl.gz'\n",
    "n_epochs=10\n",
    "LearningRate=0.075\n",
    "LearningRateDecay=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shared_dataset(datafile):\n",
    "    f = gzip.open(datafile, 'rb')\n",
    "    data= pickle.load(f)\n",
    "    segments = pickle.load(f)\n",
    "    labels = pickle.load(f)\n",
    "    chardata=pickle.load(f)\n",
    "    f.close()\n",
    "    shared_x = theano.shared(numpy.asarray(data,dtype=theano.config.floatX),borrow=True)\n",
    "    shared_char = theano.shared(numpy.asarray(chardata,dtype=theano.config.floatX),borrow=True)\n",
    "    shared_y = theano.shared(numpy.asarray(labels,dtype=theano.config.floatX),borrow=True)\n",
    "    shared_segments = theano.shared(numpy.asarray(segments,dtype=numpy.int32),borrow=True)\n",
    "    return labels.shape[0], shared_x, shared_char, shared_y, shared_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... Loading Data\n"
     ]
    }
   ],
   "source": [
    "sys.stderr.write( '... Loading Data\\n')\n",
    "n_test, test_set_x, test_set_char_x,  test_set_y,  test_segments = shared_dataset(d+'test.dat.pcl.gz')\n",
    "n_train, train_text, train_chars, train_y , train_segments  = shared_dataset(d+'train.dat.pcl.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar('ind')   # index of an example\n",
    "input = T.matrix('x')      # Each input example is represented by a number of moving windows on the text. Words in these windows are represented by\n",
    "                                        # their index in the word dictionary\n",
    "y = T.vector('y')          # the label is an integer labels\n",
    "cha= T.tensor4('cha')      # Each word is assumed to be max 10 char long, and is repesented as 10 windows of char indicies\n",
    "lr = T.scalar('l_r', dtype=theano.config.floatX)\n",
    "mom = T.scalar('mom', dtype=theano.config.floatX)\n",
    "params=[]\n",
    "rng = numpy.random.RandomState(23455)\n",
    "\n",
    "n_hidden=[]\n",
    "HL=hiddenLayerSizes.split(\"_\")\n",
    "for h in HL:\n",
    "    n_hidden.append(int(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "size=inputSize*(wordVecLen+CCLayerSize)          # number of input to the convolution Layer\n",
    "f = gzip.open(vecfile, 'rb')\n",
    "WV=theano.shared(numpy.asarray(pickle.load(f), dtype=numpy.float32),borrow=True)\n",
    "f.close()\n",
    "params.append(WV)\n",
    "combinedInp=WV[T.cast(input,'int32'),]               #The input the layer are indicies in the word/char vector perameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(CCLayerSize>0):                                                          # Set up the char convolution layer\n",
    "    charDictSize=100\n",
    "    CWV=theano.shared(numpy.asarray( rng.uniform(low=-1.25, high=1.7, size=(charDictSize,CVectorSize)), dtype=theano.config.floatX), name='CWV', borrow=True)\n",
    "    params.append(CWV)\n",
    "    cha_in=(2*CCContext+1)*CVectorSize\n",
    "    lm=numpy.sqrt(6. / (cha_in + CCLayerSize))\n",
    "    Cha_W = theano.shared(value=numpy.asarray(rng.uniform(low=-lm, high=lm, size=(cha_in, CCLayerSize)), dtype=theano.config.floatX),name='Cha_W', borrow=True)\n",
    "    Cha_b = theano.shared(value=numpy.zeros((CCLayerSize,), dtype=theano.config.floatX), name='Cha_b', borrow=True)\n",
    "    \n",
    "    Cha_output = T.tanh(T.dot(T.reshape(CWV[T.cast(cha,'int32'),], [cha.shape[0],cha.shape[1],cha.shape[2], cha_in]), Cha_W) + Cha_b)\n",
    "    CharConvOutput= T.max(Cha_output,axis=2)\n",
    "    params.append(Cha_W)\n",
    "    params.append(Cha_b)\n",
    "    L1 = abs(CWV).sum() + abs(Cha_W).sum()\n",
    "    L2_sqr = (CWV ** 2).sum()+(Cha_W ** 2).sum()\n",
    "    combinedInp=T.concatenate([combinedInp, CharConvOutput], axis=2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedInp=T.reshape(combinedInp, [input.shape[0], size])\n",
    "hiddenLayer=[]\n",
    "hiddenLayer.append(HiddenLayer(rng, combinedInp, size , n_hidden[0]))\n",
    "ConvOut=T.max(hiddenLayer[0].output,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1= theano.function([index],\n",
    "                    outputs=[cha, input, y],\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "\n",
    "f2= theano.function([index],\n",
    "                    outputs=combinedInp,\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "f3= theano.function([index],\n",
    "                    outputs=ConvOut,\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hl=hiddenLayer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "W"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FeatureInputs=ConvOut\n",
    "\n",
    "for layer in hiddenLayer:\n",
    "    L1 = abs(layer.W).sum() \n",
    "    L2_sqr = (layer.W ** 2).sum()\n",
    "    params += layer.params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logRegressionLayer = LogisticRegression(FeatureInputs, n_hidden[len(n_hidden)-1]+globalfeCount, n_out)\n",
    "L1=abs(logRegressionLayer.W).sum()\n",
    "L2_sqr=(logRegressionLayer.W ** 2).sum()\n",
    "params+=logRegressionLayer.params\n",
    "negative_log_likelihood = logRegressionLayer.negative_log_likelihood\n",
    "errors = logRegressionLayer.errors\n",
    "p_y_given_x=logRegressionLayer.p_y_given_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DisconnectedInputError",
     "evalue": "grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: W",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisconnectedInputError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5fc95b27807f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_to_app_to_idx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mhandle_disconnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mgrad_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisconnected_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mhandle_disconnected\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdisconnected_inputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDisconnectedInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[0;31mDisconnectedInputError\u001b[0m: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: W"
     ]
    }
   ],
   "source": [
    "cost=negative_log_likelihood(T.cast(y,'int32')) + L1_reg * L1  + L2_reg * L2_sqr\n",
    "gparams = []\n",
    "for param in params:\n",
    "    gparam = T.grad(cost, param)\n",
    "    gparams.append(gparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{add,no_inplace}.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DisconnectedInputError",
     "evalue": "grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: W",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisconnectedInputError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f8a0bb320a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_to_app_to_idx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mhandle_disconnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mgrad_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisconnected_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/gradient.pyc\u001b[0m in \u001b[0;36mhandle_disconnected\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdisconnected_inputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDisconnectedInputError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[0;31mDisconnectedInputError\u001b[0m: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: W"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "updates = []\n",
    "for param, gparam in zip(params, gparams):\n",
    "    updates.append((param, param - lr * gparam))\n",
    "\n",
    "\n",
    "\n",
    "f4= theano.function([index],\n",
    "                    outputs=cost,\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "train_model = theano.function([index, lr],\n",
    "                    outputs=cost ,\n",
    "                    updates=updates,\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "training_error = theano.function([index],\n",
    "                    outputs=errors(T.cast(y,'int32')),\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: train_text[train_segments[index]: train_segments[index+1]],\n",
    "                            cha: train_chars[T.cast(train_text[train_segments[index] : train_segments[index + 1]],'int32')],\n",
    "                            y: train_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "f11= theano.function([index],\n",
    "                    outputs=[cha, input, y, fe],\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: test_set_x[test_segments[index]: test_segments[index+1]],\n",
    "                            cha: test_set_char_x[test_segments[index]: test_segments[index+1],],\n",
    "                            y: test_set_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "testing_error = theano.function([index],\n",
    "                    outputs=errors(T.cast(y,'int32')),\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: test_set_x[test_segments[index]: test_segments[index+1]],\n",
    "                            cha: test_set_char_x[test_segments[index]: test_segments[index+1],],\n",
    "                            y: test_set_y[index:index+1],\n",
    "                            })\n",
    "classDist = theano.function([index],\n",
    "                    p_y_given_x,\n",
    "                    on_unused_input='ignore',\n",
    "                    givens={input: test_set_x[test_segments[index]: test_segments[index+1]],\n",
    "                            cha: test_set_char_x[test_segments[index]: test_segments[index+1],],\n",
    "                            y: test_set_y[index:index+1],\n",
    "                            })\n",
    "\n",
    "def RunTrainingEpoch(n_train, learning_rate):\n",
    "    train_losses=[train_model(i, learning_rate) for i in xrange(n_train)]\n",
    "    return  numpy.mean(train_losses)#, numpy.mean(errors), numpy.sum(errors) \n",
    "\n",
    "\n",
    "def train_error(n_train):\n",
    "    train_losses = numpy.mean([training_error(i) for i in xrange(n_train)])\n",
    "    \n",
    "def test_error(n_test):\n",
    "    err=[]\n",
    "    for i in xrange(n_test):\n",
    "        e=testing_error(i)\n",
    "        err.extend(e)\n",
    "    return numpy.mean(err)\n",
    "\n",
    "epoch = 0\n",
    "while (epoch < n_epochs):\n",
    "    epoch = epoch + 1\n",
    "    sys.stderr.write( '... Training epoch '+str(epoch)+\"\\n\")\n",
    "    train_losses=RunTrainingEpoch(n_train, LearningRate)\n",
    "    #train_losses = train_error(n_train)\n",
    "    if(n_test>0):\n",
    "        test_losses = test_error(n_test)\n",
    "        sys.stderr.write('epoch %i, tr loss %f te errors %f lr: %f\\n' % (epoch, train_losses, test_losses*100, LearningRate))\n",
    "    else:\n",
    "        sys.stderr.write('epoch %i, tr loss %f lr: %f\\n' % (epoch, train_losses, LearningRate)) \n",
    "    LearningRate *= LearningRateDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
